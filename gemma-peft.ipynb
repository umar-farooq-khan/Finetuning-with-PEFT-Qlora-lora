{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"389cd7df53c44a199b564e1cca7cf16c":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_cda6c8423db14c6da7b5c738541d5d08","IPY_MODEL_37d58d981755438584d09fdefa2258f5","IPY_MODEL_f237213f957145d18dc2142a595ed461","IPY_MODEL_076ed662773649489ae742f2c44a9893","IPY_MODEL_6f93247278e34e05a09f1c8fb66b4c16"],"layout":"IPY_MODEL_c19211b2ba7c4bb5a23990a508ae94f8"}},"cda6c8423db14c6da7b5c738541d5d08":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d0ece6190604fb987549a586e29db50","placeholder":"​","style":"IPY_MODEL_60445075ef854a4f896af15f52687e05","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"37d58d981755438584d09fdefa2258f5":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_9fd6f3c7517c4e06a3e99bee03a94bad","placeholder":"​","style":"IPY_MODEL_52accbc18620401f943c53f71c8c46e5","value":""}},"f237213f957145d18dc2142a595ed461":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_a090b0f6a2e0426bad9f83584c920e81","style":"IPY_MODEL_bd731b3730114b009e1b602cbb155f06","value":true}},"076ed662773649489ae742f2c44a9893":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_b058d2bf88074a999032eab972a79bac","style":"IPY_MODEL_a61a66590b354e4c846941e579b4e60a","tooltip":""}},"6f93247278e34e05a09f1c8fb66b4c16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a64d81b8ed24faeb229480fc962342c","placeholder":"​","style":"IPY_MODEL_abe7c1d2da444dc0a26fb29ad6ad3a36","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"c19211b2ba7c4bb5a23990a508ae94f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"4d0ece6190604fb987549a586e29db50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"60445075ef854a4f896af15f52687e05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fd6f3c7517c4e06a3e99bee03a94bad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52accbc18620401f943c53f71c8c46e5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a090b0f6a2e0426bad9f83584c920e81":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd731b3730114b009e1b602cbb155f06":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b058d2bf88074a999032eab972a79bac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a61a66590b354e4c846941e579b4e60a":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"5a64d81b8ed24faeb229480fc962342c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abe7c1d2da444dc0a26fb29ad6ad3a36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7740647,"sourceType":"datasetVersion","datasetId":4524247},{"sourceId":7740681,"sourceType":"datasetVersion","datasetId":4524266}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<img src=\"https://storage.googleapis.com/gweb-uniblog-publish-prod/images/gemma-header.width-1200.format-webp.webp\" width=\"100%\">\n\n## Instruct Fine-tuning [Gemma](https://blog.google/technology/developers/gemma-open-models/) using qLora and Supervise Finetuning\n\nThis is a comprahensive notebook and tutorial on how to fine tune the `gemma-7b-it` Model\n\nAll the code will be available on my Github. Do drop by and give a follow and a star.\n[adithya-s-k](https://github.com/adithya-s-k)\n\\\n[Github Code](https://github.com/adithya-s-k/LLM-Cookbook/blob/main/LLMs/Gemma/finetune-gemma.ipynb)\n\nI also post content about LLMs and what I have been working on Twitter.\n[AdithyaSK (@adithya_s_k) / X](https://twitter.com/adithya_s_k)","metadata":{"id":"U2a9EPtz9LZB"}},{"cell_type":"markdown","source":"## Prerequisites\n\nBefore delving into the fine-tuning process, ensure that you have the following prerequisites in place:\n\n1. **GPU**: [gemma-2b](https://huggingface.co/google/gemma-2b) - can be finetuned on T4(free google colab) while [gemma-7b](https://huggingface.co/google/gemma-7b) requires an A100 GPU.\n2. **Python Packages**: Ensure that you have the necessary Python packages installed. You can use the following commands to install them:\n\nLet's begin by checking if your GPU is correctly detected:","metadata":{"id":"ogEJiuuc9LZD"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"gpun6Z9n9LZD","outputId":"e4649e0e-7d5e-46b9-8bf3-d4bc4e2fd8e3","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2024-03-03T02:43:15.779704Z","iopub.execute_input":"2024-03-03T02:43:15.780086Z","iopub.status.idle":"2024-03-03T02:43:16.956810Z","shell.execute_reply.started":"2024-03-03T02:43:15.780054Z","shell.execute_reply":"2024-03-03T02:43:16.955673Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Sun Mar  3 02:43:16 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   34C    P0              25W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\ndf= pd.read_csv('https://raw.githubusercontent.com/umar-farooq-khan/av_data/main/gemmacsv.csv')\ndf","metadata":{"execution":{"iopub.status.busy":"2024-03-03T02:43:16.959163Z","iopub.execute_input":"2024-03-03T02:43:16.959545Z","iopub.status.idle":"2024-03-03T02:43:17.774181Z","shell.execute_reply.started":"2024-03-03T02:43:16.959509Z","shell.execute_reply":"2024-03-03T02:43:17.773027Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0                                              input  \\\n0             0  AV_manufacturer:Aimotive , Inc, AVyear:2010.0,...   \n1             1  AV_manufacturer:Apple Inc, AVyear:2017.0, AVma...   \n2             2  AV_manufacturer:Aurora Innovation Inc, AVyear:...   \n3             3  AV_manufacturer:Cruise LLC, AVyear:2020.0, AVm...   \n4             4  AV_manufacturer:Cruise LLC, AVyear:2020.0, AVm...   \n..          ...                                                ...   \n322         322  AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...   \n323         323  AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...   \n324         324  AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...   \n325         325  AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...   \n326         326  AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...   \n\n                                                output  \\\n0    On 09/16/2019 at approximately 9:55 AM, an Aim...   \n1    On September 19th, 2019 at 7:58am an Apple tes...   \n2    On 01/10/19 at 2:52pm, an Aurora vehicle, whil...   \n3    A Cruise autonomous vehicle (“Cruise AV”), ope...   \n4    A Cruise autonomous vehicle (“Cruise AV”), ope...   \n..                                                 ...   \n322  A Zoox vehicle in autonomous mode was stopped ...   \n323  A Zoox vehicle in manual mode was stopped at a...   \n324  A Zoox vehicle in autonomous mode was proceedi...   \n325  A Zoox vehicle in autonomous mode was stopped ...   \n326  A Zoox vehicle in autonomous mode was proceedi...   \n\n                                           instruction  \\\n0    please prepare the crash report when given wit...   \n1    please prepare the crash report when given wit...   \n2    please prepare the crash report when given wit...   \n3    please prepare the crash report when given wit...   \n4    please prepare the crash report when given wit...   \n..                                                 ...   \n322  please prepare the crash report when given wit...   \n323  please prepare the crash report when given wit...   \n324  please prepare the crash report when given wit...   \n325  please prepare the crash report when given wit...   \n326  please prepare the crash report when given wit...   \n\n                                                prompt  \n0    <start_of_turn>user Below is an instruction th...  \n1    <start_of_turn>user Below is an instruction th...  \n2    <start_of_turn>user Below is an instruction th...  \n3    <start_of_turn>user Below is an instruction th...  \n4    <start_of_turn>user Below is an instruction th...  \n..                                                 ...  \n322  <start_of_turn>user Below is an instruction th...  \n323  <start_of_turn>user Below is an instruction th...  \n324  <start_of_turn>user Below is an instruction th...  \n325  <start_of_turn>user Below is an instruction th...  \n326  <start_of_turn>user Below is an instruction th...  \n\n[327 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>input</th>\n      <th>output</th>\n      <th>instruction</th>\n      <th>prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>AV_manufacturer:Aimotive , Inc, AVyear:2010.0,...</td>\n      <td>On 09/16/2019 at approximately 9:55 AM, an Aim...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>AV_manufacturer:Apple Inc, AVyear:2017.0, AVma...</td>\n      <td>On September 19th, 2019 at 7:58am an Apple tes...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>AV_manufacturer:Aurora Innovation Inc, AVyear:...</td>\n      <td>On 01/10/19 at 2:52pm, an Aurora vehicle, whil...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>AV_manufacturer:Cruise LLC, AVyear:2020.0, AVm...</td>\n      <td>A Cruise autonomous vehicle (“Cruise AV”), ope...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>AV_manufacturer:Cruise LLC, AVyear:2020.0, AVm...</td>\n      <td>A Cruise autonomous vehicle (“Cruise AV”), ope...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>322</th>\n      <td>322</td>\n      <td>AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...</td>\n      <td>A Zoox vehicle in autonomous mode was stopped ...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n    <tr>\n      <th>323</th>\n      <td>323</td>\n      <td>AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...</td>\n      <td>A Zoox vehicle in manual mode was stopped at a...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n    <tr>\n      <th>324</th>\n      <td>324</td>\n      <td>AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...</td>\n      <td>A Zoox vehicle in autonomous mode was proceedi...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n    <tr>\n      <th>325</th>\n      <td>325</td>\n      <td>AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...</td>\n      <td>A Zoox vehicle in autonomous mode was stopped ...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n    <tr>\n      <th>326</th>\n      <td>326</td>\n      <td>AV_manufacturer:Zoox Inc, AVyear:2016.0, AVmak...</td>\n      <td>A Zoox vehicle in autonomous mode was proceedi...</td>\n      <td>please prepare the crash report when given wit...</td>\n      <td>&lt;start_of_turn&gt;user Below is an instruction th...</td>\n    </tr>\n  </tbody>\n</table>\n<p>327 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Step 2 - Model loading\nWe'll load the model using QLoRA quantization to reduce the usage of memory\n","metadata":{"id":"c1kSpHz69LZE"}},{"cell_type":"code","source":"!pip3 install -q -U bitsandbytes==0.42.0\n!pip3 install -q -U peft==0.8.2\n!pip3 install -q -U trl==0.7.10\n!pip3 install -q -U accelerate==0.27.1\n!pip3 install -q -U datasets==2.17.0\n!pip3 install -q -U transformers==4.38.0","metadata":{"id":"Cgr6OCgL9LZF","execution":{"iopub.status.busy":"2024-03-03T02:43:17.775565Z","iopub.execute_input":"2024-03-03T02:43:17.775870Z","iopub.status.idle":"2024-03-03T02:45:08.161507Z","shell.execute_reply.started":"2024-03-03T02:43:17.775845Z","shell.execute_reply":"2024-03-03T02:45:08.160311Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.0 which is incompatible.\nbeatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.1.2 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.0 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ngcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.2.0 which is incompatible.\ns3fs 2024.2.0 requires fsspec==2024.2.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)","metadata":{"id":"dmS9qsCi9LZF","execution":{"iopub.status.busy":"2024-03-03T02:45:08.164939Z","iopub.execute_input":"2024-03-03T02:45:08.165632Z","iopub.status.idle":"2024-03-03T02:45:13.622969Z","shell.execute_reply.started":"2024-03-03T02:45:08.165590Z","shell.execute_reply":"2024-03-03T02:45:13.621981Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Now we specify the model ID and then we load it with our previously defined quantization configuration.Now we specify the model ID and then we load it with our previously defined quantization configuration.","metadata":{"id":"0XHxrQkv9LZF"}},{"cell_type":"code","source":"# if you are using google colab\n\n# import os\n# from google.colab import userdata\n# os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')","metadata":{"id":"dV3SJMCX9LZG","execution":{"iopub.status.busy":"2024-03-03T02:45:13.624273Z","iopub.execute_input":"2024-03-03T02:45:13.624676Z","iopub.status.idle":"2024-03-03T02:45:13.628702Z","shell.execute_reply.started":"2024-03-03T02:45:13.624652Z","shell.execute_reply":"2024-03-03T02:45:13.627721Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"id":"JKb7r0mn9LZG","outputId":"e494be61-d846-42d8-d6b5-47f4cb14a6db","colab":{"base_uri":"https://localhost:8080/","height":387,"referenced_widgets":["389cd7df53c44a199b564e1cca7cf16c","cda6c8423db14c6da7b5c738541d5d08","37d58d981755438584d09fdefa2258f5","f237213f957145d18dc2142a595ed461","076ed662773649489ae742f2c44a9893","6f93247278e34e05a09f1c8fb66b4c16","c19211b2ba7c4bb5a23990a508ae94f8","4d0ece6190604fb987549a586e29db50","60445075ef854a4f896af15f52687e05","9fd6f3c7517c4e06a3e99bee03a94bad","52accbc18620401f943c53f71c8c46e5","a090b0f6a2e0426bad9f83584c920e81","bd731b3730114b009e1b602cbb155f06","b058d2bf88074a999032eab972a79bac","a61a66590b354e4c846941e579b4e60a","5a64d81b8ed24faeb229480fc962342c","abe7c1d2da444dc0a26fb29ad6ad3a36"]},"execution":{"iopub.status.busy":"2024-03-03T02:45:13.629922Z","iopub.execute_input":"2024-03-03T02:45:13.630220Z","iopub.status.idle":"2024-03-03T02:45:13.851805Z","shell.execute_reply.started":"2024-03-03T02:45:13.630197Z","shell.execute_reply":"2024-03-03T02:45:13.850948Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38460c2bb42942d6b5d7412104a383a5"}},"metadata":{}}]},{"cell_type":"code","source":"model_id = \"google/gemma-7b-it\"\n# model_id = \"google/gemma-7b\"\n#model_id = \"google/gemma-2b-it\"\n# model_id = \"google/gemma-2b\"\n\nmodel = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0})\ntokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)","metadata":{"id":"eOkgAApw9LZG","outputId":"a7a78ccf-db8e-49db-feb8-67745a3e4ae5","colab":{"base_uri":"https://localhost:8080/","height":738},"execution":{"iopub.status.busy":"2024-03-03T02:47:34.752721Z","iopub.execute_input":"2024-03-03T02:47:34.753137Z","iopub.status.idle":"2024-03-03T02:50:28.204627Z","shell.execute_reply.started":"2024-03-03T02:47:34.753096Z","shell.execute_reply":"2024-03-03T02:50:28.203799Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43ea4e7ee99140deabe35971e8849f78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed49de7ab044462a708af2840fbff9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b81609de513e4d68b66d5633f5febc76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196bf73a70d34bd1900794417ef2a112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00456117cb12481bafb7ddf0778d5a7d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daee5460381045f7b95325773c10a662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67d70c46ec364bf8acbd1ca940c31221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8043cdd1cc44b3d9ba1989d4bd0983e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d584206ec6244a1ba11d85772265d4c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a150e1bf37b84cbb9aabc5822270b1ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caaafb6514614a5a8738a21387585d67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93c47859070f4a1f8d5591cc489edd5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/888 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e7ca690ee5c40e4b0516e91200c67eb"}},"metadata":{}}]},{"cell_type":"code","source":"def get_completion(query: str, model, tokenizer) -> str:\n  device = \"cuda:0\"\n\n  prompt_template = \"\"\"\n  <bos><start_of_turn>user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  {query}\n  <end_of_turn>\\n<start_of_turn>model\n\n\n  \"\"\"\n  prompt = prompt_template.format(query=query)\n\n  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n\n  model_inputs = encodeds.to(device)\n\n\n  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n  # decoded = tokenizer.batch_decode(generated_ids)\n  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n  return (decoded)","metadata":{"id":"OFEgXHv19LZH","execution":{"iopub.status.busy":"2024-03-03T02:50:28.206256Z","iopub.execute_input":"2024-03-03T02:50:28.206717Z","iopub.status.idle":"2024-03-03T02:50:28.213607Z","shell.execute_reply.started":"2024-03-03T02:50:28.206691Z","shell.execute_reply":"2024-03-03T02:50:28.212600Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=model, tokenizer=tokenizer)\nprint(result)","metadata":{"id":"u0uJKj2F9LZI","execution":{"iopub.status.busy":"2024-03-03T02:50:28.214698Z","iopub.execute_input":"2024-03-03T02:50:28.214979Z","iopub.status.idle":"2024-03-03T02:51:16.640287Z","shell.execute_reply.started":"2024-03-03T02:50:28.214955Z","shell.execute_reply":"2024-03-03T02:51:16.639242Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n2024-03-03 02:50:31.452404: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-03 02:50:31.452505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-03 02:50:31.604029: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n  user\n  Below is an instruction that describes a task. Write a response that appropriately completes the request.\n  code the fibonacci series in python using reccursion\n  \nmodel\n\n\n  ```python\ndef fibonacci(n):\n  \"\"\"Calculates the nth Fibonacci number using recursion.\n\n  The Fibonacci sequence is a series of numbers in which each number is the sum of the previous two numbers in the sequence, starting from the first two numbers (0 and 1).\n\n  Args:\n    n: The number of the Fibonacci number to calculate.\n\n  Returns:\n    The nth Fibonacci number.\n  \"\"\"\n\n  # Base case: If n is 0 or 1, return the corresponding number.\n  if n == 0 or n == 1:\n    return n\n\n  # Recursive case: Otherwise, calculate the nth Fibonacci number by adding the previous two numbers in the sequence.\n  else:\n    return fibonacci(n-1) + fibonacci(n-2)\n```\n\n**Explanation:**\n\n* The `fibonacci` function takes an integer `n` as input.\n* The function first checks if `n` is 0 or 1, and if it is, it returns `n` itself. This is the base case of the recursion.\n* Otherwise, it calculates the nth Fibonacci number by adding the previous two numbers in the sequence, which are calculated recursively by calling the function `fibonacci` with `n-1` and `n-2` as arguments.\n* The results of these recursive calls are then added and returned as the nth Fibonacci number.\n\n**Example Usage:**\n\n```python\n# Print the Fibonacci number at position 10\nprint(fibonacci(10))\n\n# Output: 5\n```\n\n**Note:**\n\n* The Fibonacci sequence is a recursive sequence, meaning that each number in the sequence is calculated by adding the previous two numbers in the sequence.\n* This function uses recursion to calculate the nth Fibonacci number, which means that it calls itself repeatedly until the base case is reached.\n* Recursion can be a powerful technique for solving problems that involve iterating over a sequence or solving problems that require a solution that depends on itself.\n","output_type":"stream"}]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3 - Load dataset for finetuning","metadata":{"id":"UAbCY75l9LZI"}},{"cell_type":"markdown","source":"### Lets Load the Dataset\n\nFor this tutorial, we will fine-tune Mistral 7B Instruct for code generation.\n\nWe will be using this [dataset](https://huggingface.co/datasets/TokenBender/code_instructions_122k_alpaca_style) which is curated by [TokenBender (e/xperiments)](https://twitter.com/4evaBehindSOTA) and is an excellent data source for fine-tuning models for code generation. It follows the alpaca style of instructions, which is an excellent starting point for this task. The dataset structure should resemble the following:\n\n```json\n{\n  \"instruction\": \"Create a function to calculate the sum of a sequence of integers.\",\n  \"input\": \"[1, 2, 3, 4, 5]\",\n  \"output\": \"# Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum\"\n}\n```","metadata":{"id":"YWOrFrrR9LZJ"}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\", split=\"train\")\ndataset","metadata":{"id":"s5figec19LZJ","execution":{"iopub.status.busy":"2024-03-03T02:51:16.642438Z","iopub.execute_input":"2024-03-03T02:51:16.642964Z","iopub.status.idle":"2024-03-03T02:51:19.038724Z","shell.execute_reply.started":"2024-03-03T02:51:16.642936Z","shell.execute_reply":"2024-03-03T02:51:19.036534Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenBender/code_instructions_122k_alpaca_style\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m dataset\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ruff: noqa\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.17.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:67\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_files\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:29\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, List, Optional, Union\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/parquet/__init__.py:20\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Licensed to the Apache Software Foundation (ASF) under one\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# or more contributor license agreements.  See the NOTICE file\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# distributed with this work for additional information\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# flake8: noqa\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/parquet/core.py:33\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_parquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_parquet\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe pyarrow installation is not built with support \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor the Parquet file format (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(exc)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyarrow/_parquet.pyx:1\u001b[0m, in \u001b[0;36minit pyarrow._parquet\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject"],"ename":"ValueError","evalue":"pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject","output_type":"error"}]},{"cell_type":"code","source":"df = dataset.to_pandas()\ndf.head(10)","metadata":{"id":"asT5an5s9LZJ","execution":{"iopub.status.busy":"2024-03-03T02:51:19.039402Z","iopub.status.idle":"2024-03-03T02:51:19.039728Z","shell.execute_reply.started":"2024-03-03T02:51:19.039570Z","shell.execute_reply":"2024-03-03T02:51:19.039583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Instruction Fintuning - Prepare the dataset under the format of \"prompt\" so the model can better understand :\n1. the function generate_prompt : take the instruction and output and generate a prompt\n2. shuffle the dataset\n3. tokenizer the dataset","metadata":{"id":"xzpzSCy89LZJ"}},{"cell_type":"markdown","source":"### Formatting the Dataset\n\nNow, let's format the dataset in the required [gemma instruction formate](https://huggingface.co/google/gemma-7b-it).\n\n> Many tutorials and blogs skip over this part, but I feel this is a really important step.\n\n```\n<start_of_turn>user What is your favorite condiment? <end_of_turn>\n<start_of_turn>model Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavor to whatever I'm cooking up in the kitchen!<end_of_turn>\n```\n\nYou can use the following code to process your dataset and create a JSONL file in the correct format:","metadata":{"id":"wkuxmkZp9LZK"}},{"cell_type":"code","source":"def generate_prompt(data_point):\n    \"\"\"Gen. input text based on a prompt, task instruction, (context info.), and answer\n\n    :param data_point: dict: Data point\n    :return: dict: tokenzed prompt\n    \"\"\"\n    prefix_text = 'Below is an instruction that describes a task. Write a response that ' \\\n               'appropriately completes the request.\\n\\n'\n    # Samples with additional context into.\n    if data_point['input']:\n        text = f\"\"\"<bos><start_of_turn>user {prefix_text} {data_point[\"instruction\"]} here are the inputs {data_point[\"input\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n    # Without\n    else:\n        text = f\"\"\"<start_of_turn>user {prefix_text} {data_point[\"instruction\"]} <end_of_turn>\\n<start_of_turn>model{data_point[\"output\"]} <end_of_turn>\"\"\"\n    return text\n\n# add the \"prompt\" column in the dataset\ntext_column = [generate_prompt(data_point) for data_point in dataset]\ndataset = dataset.add_column(\"prompt\", text_column)","metadata":{"id":"K3zh4ugj9LZK","execution":{"iopub.status.busy":"2024-03-03T02:51:19.041229Z","iopub.status.idle":"2024-03-03T02:51:19.041578Z","shell.execute_reply.started":"2024-03-03T02:51:19.041413Z","shell.execute_reply":"2024-03-03T02:51:19.041427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll need to tokenize our data so the model can understand.\n","metadata":{"id":"_uOzv7iB9LZK"}},{"cell_type":"code","source":"dataset = dataset.shuffle(seed=1234)  # Shuffle dataset here\ndataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)","metadata":{"id":"Q3DNFbS79LZL","execution":{"iopub.status.busy":"2024-03-03T02:51:19.043135Z","iopub.status.idle":"2024-03-03T02:51:19.043444Z","shell.execute_reply.started":"2024-03-03T02:51:19.043294Z","shell.execute_reply":"2024-03-03T02:51:19.043307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split dataset into 90% for training and 10% for testing","metadata":{"id":"daYHy6ce9LZL"}},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.2)\ntrain_data = dataset[\"train\"]\ntest_data = dataset[\"test\"]","metadata":{"id":"P-KRGR_H9LZL","execution":{"iopub.status.busy":"2024-03-03T02:51:19.044346Z","iopub.status.idle":"2024-03-03T02:51:19.044647Z","shell.execute_reply.started":"2024-03-03T02:51:19.044497Z","shell.execute_reply":"2024-03-03T02:51:19.044510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### After Formatting, We should get something like this\n\n```json\n{\n\"text\":\"<start_of_turn>user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] <end_of_turn>\n<start_of_turn>model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum <end_of_turn>\",\n\"instruction\":\"Create a function to calculate the sum of a sequence of integers\",\n\"input\":\"[1, 2, 3, 4, 5]\",\n\"output\":\"# Python code def sum_sequence(sequence): sum = 0 for num in,\n sequence: sum += num return sum\",\n\"prompt\":\"<start_of_turn>user Create a function to calculate the sum of a sequence of integers. here are the inputs [1, 2, 3, 4, 5] <end_of_turn>\n<start_of_turn>model # Python code def sum_sequence(sequence): sum = 0 for num in sequence: sum += num return sum <end_of_turn>\"\n\n}\n```\n\nWhile using SFT (**[Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/main/en/sft_trainer)**) for fine-tuning, we will be only passing in the “text” column of the dataset for fine-tuning.","metadata":{"id":"3wM6XxRP9LZL"}},{"cell_type":"code","source":"print(test_data)","metadata":{"id":"2BlI1gp19LZM","execution":{"iopub.status.busy":"2024-03-03T02:51:19.046135Z","iopub.status.idle":"2024-03-03T02:51:19.046580Z","shell.execute_reply.started":"2024-03-03T02:51:19.046350Z","shell.execute_reply":"2024-03-03T02:51:19.046369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 4 - Apply Lora  \nHere comes the magic with peft! Let's load a PeftModel and specify that we are going to use low-rank adapters (LoRA) using get_peft_model utility function and  the prepare_model_for_kbit_training method from PEFT.","metadata":{"id":"KzFZr8zv9LZM"}},{"cell_type":"code","source":"from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)","metadata":{"id":"r9n_e_639LZM","execution":{"iopub.status.busy":"2024-03-03T02:51:19.047651Z","iopub.status.idle":"2024-03-03T02:51:19.048098Z","shell.execute_reply.started":"2024-03-03T02:51:19.047871Z","shell.execute_reply":"2024-03-03T02:51:19.047891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(model)","metadata":{"id":"gYiblN9t9LZN","execution":{"iopub.status.busy":"2024-03-03T02:51:19.049487Z","iopub.status.idle":"2024-03-03T02:51:19.049925Z","shell.execute_reply.started":"2024-03-03T02:51:19.049697Z","shell.execute_reply":"2024-03-03T02:51:19.049715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import bitsandbytes as bnb\ndef find_all_linear_names(model):\n  cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n  lora_module_names = set()\n  for name, module in model.named_modules():\n    if isinstance(module, cls):\n      names = name.split('.')\n      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names: # needed for 16-bit\n      lora_module_names.remove('lm_head')\n  return list(lora_module_names)","metadata":{"id":"VJjGPtKH9LZN","execution":{"iopub.status.busy":"2024-03-03T02:51:19.051229Z","iopub.status.idle":"2024-03-03T02:51:19.051602Z","shell.execute_reply.started":"2024-03-03T02:51:19.051438Z","shell.execute_reply":"2024-03-03T02:51:19.051453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"modules = find_all_linear_names(model)\nprint(modules)","metadata":{"id":"HrtnP54d9LZO","execution":{"iopub.status.busy":"2024-03-03T02:51:19.053196Z","iopub.status.idle":"2024-03-03T02:51:19.053540Z","shell.execute_reply.started":"2024-03-03T02:51:19.053376Z","shell.execute_reply":"2024-03-03T02:51:19.053391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model\n\nlora_config = LoraConfig(\n    r=64,\n    lora_alpha=32,\n    target_modules=modules,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\n\nmodel = get_peft_model(model, lora_config)","metadata":{"id":"mMjCjQmI9LZO","execution":{"iopub.status.busy":"2024-03-03T02:51:19.054360Z","iopub.status.idle":"2024-03-03T02:51:19.054657Z","shell.execute_reply.started":"2024-03-03T02:51:19.054508Z","shell.execute_reply":"2024-03-03T02:51:19.054521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainable, total = model.get_nb_trainable_parameters()\nprint(f\"Trainable: {trainable} | total: {total} | Percentage: {trainable/total*100:.4f}%\")","metadata":{"id":"AvDmypQ09LZO","execution":{"iopub.status.busy":"2024-03-03T02:51:19.055936Z","iopub.status.idle":"2024-03-03T02:51:19.056292Z","shell.execute_reply.started":"2024-03-03T02:51:19.056103Z","shell.execute_reply":"2024-03-03T02:51:19.056138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5 - Run the training!","metadata":{"id":"MUwzOErN9LZO"}},{"cell_type":"markdown","source":"Setting the training arguments:\n* for the reason of demo, we just ran it for few steps (100) just to showcase how to use this integration with existing tools on the HF ecosystem.","metadata":{"id":"BMroD1ch9LZP"}},{"cell_type":"code","source":"# import transformers\n\n# tokenizer.pad_token = tokenizer.eos_token\n\n\n# trainer = transformers.Trainer(\n#     model=model,\n#     train_dataset=train_data,\n#     eval_dataset=test_data,\n#     args=transformers.TrainingArguments(\n#         per_device_train_batch_size=1,\n#         gradient_accumulation_steps=4,\n#         warmup_steps=0.03,\n#         max_steps=5,\n#         learning_rate=2e-4,\n#         fp16=True,\n#         logging_steps=1,\n#         output_dir=\"outputs_mistral_b_finance_finetuned_test\",\n#         optim=\"paged_adamw_8bit\",\n#         save_strategy=\"epoch\",\n#     ),\n#     data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n# )\n","metadata":{"id":"eUUfL1MP9LZP","execution":{"iopub.status.busy":"2024-03-03T02:51:19.057334Z","iopub.status.idle":"2024-03-03T02:51:19.057656Z","shell.execute_reply.started":"2024-03-03T02:51:19.057498Z","shell.execute_reply":"2024-03-03T02:51:19.057512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fine-Tuning with qLora and Supervised Fine-Tuning\n\nWe're ready to fine-tune our model using qLora. For this tutorial, we'll use the `SFTTrainer` from the `trl` library for supervised fine-tuning. Ensure that you've installed the `trl` library as mentioned in the prerequisites.","metadata":{"id":"3CKAm_A-9LZP"}},{"cell_type":"code","source":"#new code using SFTTrainer\nimport transformers\n\nfrom trl import SFTTrainer\n\ntokenizer.pad_token = tokenizer.eos_token\ntorch.cuda.empty_cache()\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_data,\n    eval_dataset=test_data,\n    dataset_text_field=\"prompt\",\n    peft_config=lora_config,\n    args=transformers.TrainingArguments(\n        per_device_train_batch_size=1,\n        gradient_accumulation_steps=4,\n        warmup_steps=0.03,\n        max_steps=10,\n        learning_rate=2e-4,\n        logging_steps=1,\n        output_dir=\"outputs\",\n        optim=\"paged_adamw_8bit\",\n        save_strategy=\"epoch\",\n    ),\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n)","metadata":{"id":"aF3UOmEI9LZP","execution":{"iopub.status.busy":"2024-03-03T02:51:19.059061Z","iopub.status.idle":"2024-03-03T02:51:19.059512Z","shell.execute_reply.started":"2024-03-03T02:51:19.059292Z","shell.execute_reply":"2024-03-03T02:51:19.059311Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lets start training","metadata":{"id":"JNuFGms89LZQ"}},{"cell_type":"code","source":"model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\ntrainer.train()","metadata":{"id":"FqtTmqxA9LZQ","execution":{"iopub.status.busy":"2024-03-03T02:51:19.061047Z","iopub.status.idle":"2024-03-03T02:51:19.061426Z","shell.execute_reply.started":"2024-03-03T02:51:19.061241Z","shell.execute_reply":"2024-03-03T02:51:19.061256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" Share adapters on the 🤗 Hub","metadata":{"id":"_ofUnBjU9LZZ"}},{"cell_type":"code","source":"new_model = \"gemma-Code-Instruct-Finetune-test_7b\" #Name of the model you will be pushing to huggingface model hub","metadata":{"id":"zBl2mKir9LZa","execution":{"iopub.status.busy":"2024-03-03T02:51:19.062829Z","iopub.status.idle":"2024-03-03T02:51:19.063158Z","shell.execute_reply.started":"2024-03-03T02:51:19.062983Z","shell.execute_reply":"2024-03-03T02:51:19.062996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)","metadata":{"id":"DF-oJEL39LZa","execution":{"iopub.status.busy":"2024-03-03T02:51:19.064206Z","iopub.status.idle":"2024-03-03T02:51:19.064507Z","shell.execute_reply.started":"2024-03-03T02:51:19.064357Z","shell.execute_reply":"2024-03-03T02:51:19.064370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map={\"\": 0},\n)\nmerged_model= PeftModel.from_pretrained(base_model, new_model)\nmerged_model= merged_model.merge_and_unload()\n\n# Save the merged model\nmerged_model.save_pretrained(\"merged_model\",safe_serialization=True)\ntokenizer.save_pretrained(\"merged_model\")\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"id":"yRUCGrxU9LZb","execution":{"iopub.status.busy":"2024-03-03T02:51:19.065745Z","iopub.status.idle":"2024-03-03T02:51:19.066073Z","shell.execute_reply.started":"2024-03-03T02:51:19.065915Z","shell.execute_reply":"2024-03-03T02:51:19.065929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Push the model and tokenizer to the Hugging Face Model Hub\nmerged_model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"id":"KdKwkZBt9LZb","execution":{"iopub.status.busy":"2024-03-03T02:51:19.067720Z","iopub.status.idle":"2024-03-03T02:51:19.068057Z","shell.execute_reply.started":"2024-03-03T02:51:19.067894Z","shell.execute_reply":"2024-03-03T02:51:19.067909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test out Finetuned Model","metadata":{"id":"X-3vnqYB9LZc"}},{"cell_type":"code","source":"result = get_completion(query=\"code the fibonacci series in python using reccursion\", model=merged_model, tokenizer=tokenizer)\nprint(result)","metadata":{"id":"0zMh8WaL9LZc","execution":{"iopub.status.busy":"2024-03-03T02:51:19.069196Z","iopub.status.idle":"2024-03-03T02:51:19.069539Z","shell.execute_reply.started":"2024-03-03T02:51:19.069370Z","shell.execute_reply":"2024-03-03T02:51:19.069384Z"},"trusted":true},"execution_count":null,"outputs":[]}]}